{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title : Mov Bee Chatbot\n",
    "## Name : Jagriti Kumari\n",
    "### Program :  Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import colorama\n",
    "from tensorflow import keras\n",
    "from colorama import Fore, Style\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read movie lines from input file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile():\n",
    "  data = open(\"C:/Users/Jagriti/Documents/NLP/Chatbot/cornell_movie_dialogs_corpus/cornell movie-dialogs corpus/movie_lines.txt\")\n",
    "  inputArray = []\n",
    "  for line in data:\n",
    "    inputArray.append([words.strip() for words in line.split(\"+++$+++\")])\n",
    "  return inputArray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select only those movie lines for topic modeling which has more than 10 words. For Faster processing only consider first 20000 lines.\n",
    "\n",
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movie_lines(input):\n",
    "    movie_lines = []\n",
    "    count = 0\n",
    "    for line in input[:10000]:\n",
    "        count +=1\n",
    "        words = line[4].split(\" \")\n",
    "        if len(words) > 10:\n",
    "            movie_lines.append(line[4])\n",
    "    print(len(movie_lines))\n",
    "    print(count)\n",
    "    return movie_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pre-process the movie lines. Convert each movie dialogue to list of words. Cleanup dialogues like strip white spaces, remove stop words. Tokenize them and then lemmatize each words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_wrangling(extractedmovielines):\n",
    "    wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    normalized_movielines = []\n",
    "    for line in extractedmovielines:\n",
    "        line = line.lower()\n",
    "        movieline_tokens = [token.strip() for token in wtk.tokenize(line) if len(token.strip()) > 3]\n",
    "        movieline_tokens = [wnl.lemmatize(token) for token in movieline_tokens if not token.isnumeric()]\n",
    "        #movieline_tokens = [token for token in movieline_tokens if len(token) > 1]\n",
    "        movieline_tokens = [token for token in movieline_tokens if token not in stop_words]\n",
    "        movieline_tokens = list(filter(None, movieline_tokens))\n",
    "        if movieline_tokens:\n",
    "            normalized_movielines.append(movieline_tokens)\n",
    "    return normalized_movielines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create uni-gram and bi-gram words\n",
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureEngineering(norm_movielines):\n",
    "    cv = CountVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2),token_pattern=None, tokenizer=lambda doc: doc,preprocessor=lambda doc: doc)\n",
    "    cv_features = cv.fit_transform(norm_movielines)\n",
    "    # validating vocabulary size\n",
    "    vocabulary = np.array(cv.get_feature_names())\n",
    "    print(cv_features.shape)\n",
    "    return cv_features,vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform topic modelling and get topics (uni-gram and bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(cv_features,vocabulary):\n",
    "    wordset = []\n",
    "    lda_model = LatentDirichletAllocation(n_components =100,max_iter=500, max_doc_update_iter=50, learning_method='online',batch_size=1740, learning_offset=50., random_state=42, n_jobs=16)\n",
    "    movielines_topics = lda_model.fit_transform(cv_features)\n",
    "    topic_terms = lda_model.components_\n",
    "    topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:,:20]\n",
    "    topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "    topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "    for sentence in topics:\n",
    "        wordset.append(sentence.split(\", \"))\n",
    "    words = []\n",
    "    for i in range(len(wordset)):\n",
    "        for elements in wordset[i]:\n",
    "            words.append(elements)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get similar words as that of topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSynonyms(list_words):\n",
    "    list_syn = {}\n",
    "    threshold = 7\n",
    "    for word in list_words:\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lem in syn.lemmas():\n",
    "                if len(synonyms) > threshold:\n",
    "                    break\n",
    "                # Remove any special characters from synonym strings\n",
    "                lem_name = re.sub('[^a-zA-Z0-9 \\n\\.]', ' ', lem.name())\n",
    "                synonyms.append(lem_name)\n",
    "\n",
    "        list_syn[word] = list(set(synonyms))\n",
    "    return list_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Topic to a txt file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeTopic(words):\n",
    "    with open('words.txt', 'w') as f:\n",
    "        wordList = json.dumps(words)\n",
    "        f.writelines(wordList)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeSeedWords(list_syn):\n",
    "    with open('synonyms.txt', 'w') as f:\n",
    "        symList = json.dumps(list_syn)\n",
    "        f.writelines(symList)\n",
    "        #print(symList)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTopics():\n",
    "    wordFile = open('words.txt')\n",
    "    wordList = json.load(wordFile, strict=False)\n",
    "    return [word for word in wordList if len(word) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3450\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jagriti\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3450, 3992)\n"
     ]
    }
   ],
   "source": [
    "input = readFile()\n",
    "extractedmovielines = extract_movie_lines(input)\n",
    "normalized_movielines = text_wrangling(extractedmovielines)\n",
    "cv_features,vocabulary = featureEngineering(normalized_movielines)\n",
    "words = topic_model(cv_features,vocabulary)\n",
    "writeTopic(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topic detected 1969\n"
     ]
    }
   ],
   "source": [
    "topicList = readTopics()\n",
    "print(\"Total topic detected %d\"%len(topicList))\n",
    "list_syn = getSynonyms(topicList)\n",
    "writeSeedWords(list_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Intent Creator (TIRC)\n",
    "\n",
    "**Class to model intent, will be stored in json format and used for training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONEncoder\n",
    "from collections import namedtuple\n",
    "\n",
    "class Intent:\n",
    "    def __init__(self, tag:str, seedWords:list, pattern:dict, response:dict):\n",
    "        self.tag = tag\n",
    "        self.seedWords = seedWords\n",
    "        self.pattern = pattern\n",
    "        self.response = response\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'tag: {t} \\n seedWord: {s} \\n pattern: {p} \\n response: {r}'.format(t= self.tag,\n",
    "                                                                                   s= self.seedWords,\n",
    "                                                                                   p= self.pattern,\n",
    "                                                                                   r= self.response)\n",
    "\n",
    "class IntentEncoder(JSONEncoder):\n",
    "    def default(self, o):\n",
    "        return o.__dict__\n",
    "\n",
    "def customIntentDecoder(intentDict):\n",
    "    return namedtuple('X', intentDict.keys())(*intentDict.values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class to represent dialogues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dialogues:\n",
    "    def __init__(self, dialogue:str, cleanedDialogue:list, lineNo:int):\n",
    "        self.dialogue = dialogue\n",
    "        self.cleanedDialogue = cleanedDialogue\n",
    "        self.lineNo = lineNo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get set of manually created list of topic that needs to be filtered out. These are topic which represent intent like\n",
    "stuff , line, cause etc. which are very general**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSetofTopicToBeFilteredOut():\n",
    "    file = open(\"C:/kaggle/NLP Project/TopicFilter.txt\",'r')\n",
    "    topicSet = set()\n",
    "    for line in file:\n",
    "        topic = line.strip().replace(\"\\\"\",\"\").replace(\":\",\"\")\n",
    "        topicSet.add(topic)\n",
    "    print(\"Number of topic to be filtered out %d\"%len(topicSet))\n",
    "    return topicSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanDialogue(dialogue:str)->list:\n",
    "    dialogue_words = [token.strip() for token in wtk.tokenize(dialogue.lower()) if len(token.strip()) > 3]\n",
    "    dialogue_words_lemmatized = [wnl.lemmatize(token) for token in dialogue_words if token not in stop_words]\n",
    "    return dialogue_words_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read each dialogue line and convert it in Dialogues object. Store this object in a dictionary where key is dialogue no and value is Dialogues object. This map speds up creation of Intent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDialogues():\n",
    "    file = open(\"C:/Users/Jagriti/Documents/NLP/Chatbot/cornell_movie_dialogs_corpus/cornell movie-dialogs corpus/movie_lines.txt\")\n",
    "    dilogueMap = {}\n",
    "    for line in file:\n",
    "        components = line.split(\"+++$+++\")\n",
    "        lineNo = int(components[0].strip().replace(\"L\",''))\n",
    "        dialogue = components[4].strip()\n",
    "        wordList = cleanDialogue(dialogue)\n",
    "        dialogueObject = Dialogues(dialogue, wordList, lineNo)\n",
    "        dilogueMap[lineNo] = dialogueObject\n",
    "    file.close()\n",
    "    return dilogueMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopicSynonyms()->dict:\n",
    "    filePath = open('synonyms.txt')\n",
    "    data = json.load(filePath, strict=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate Jaccard similarity between two dialogues/sentences which are cleanded , tokenized and lemmatized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJaccardSimilarity(wordList1:list, wordList2:list):\n",
    "    set1 = set(wordList1)\n",
    "    set2 = set(wordList2)\n",
    "    intersection = set1.intersection(set2)\n",
    "    union = set1.union(set2)\n",
    "    return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIntent(tag:str, seedWords:list, dialogueMap:dict, jaquardThreshold):\n",
    "    intent = Intent(tag, seedWords, {}, {})\n",
    "    for lineNo in dialogueMap:\n",
    "        dialogue = dialogueMap[lineNo]\n",
    "        if dialogue.lineNo + 1 in dialogueMap and len(dialogue.cleanedDialogue) > 0:\n",
    "            jacSim = getJaccardSimilarity(seedWords, dialogue.cleanedDialogue)\n",
    "            if jacSim > jaquardThreshold:\n",
    "                intent.pattern.update({dialogue.lineNo : dialogue.dialogue})\n",
    "                intent.response.update({dialogue.lineNo: dialogueMap[dialogue.lineNo + 1].dialogue})\n",
    "\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistIntents(intentDict:dict):\n",
    "    intentJson = json.dumps(intentDict, indent=2, cls=IntentEncoder)\n",
    "    fileToWrite = open(\"intent.json\",'w')\n",
    "    fileToWrite.writelines(intentJson)\n",
    "    fileToWrite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create intent using movie dialogues and seed words. Use a JaccardSimilarity threshold of 0.1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateIntentFromTopic(similarityCofficient = 0.1):\n",
    "    dialogueMap = readDialogues()\n",
    "    synonyms = getTopicSynonyms()\n",
    "    topicFilterSet = getSetofTopicToBeFilteredOut()\n",
    "    intentDict = {}\n",
    "    print(\"creating intent .... this will take some time\")\n",
    "    count = 0\n",
    "    for tag in synonyms:\n",
    "        if(len(synonyms[tag]) > 0 and tag not in topicFilterSet):\n",
    "            count += 1\n",
    "            seedWords = synonyms[tag]\n",
    "            seedWords.append(tag)\n",
    "            intent = getIntent(tag, seedWords, dialogueMap, similarityCofficient)\n",
    "            intentDict[tag] = intent\n",
    "            if(count % 100 == 0):\n",
    "                print(\"%d intent created.\"%count)\n",
    "    print()\n",
    "    print('Total intent created %d'%len(intentDict))\n",
    "    return intentDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topic to be filtered out 485\n",
      "creating intent .... this will take some time\n",
      "100 intent created.\n",
      "200 intent created.\n",
      "300 intent created.\n",
      "400 intent created.\n",
      "500 intent created.\n",
      "600 intent created.\n",
      "700 intent created.\n",
      "800 intent created.\n",
      "\n",
      "Total intent created 857\n"
     ]
    }
   ],
   "source": [
    "intentList = generateIntentFromTopic()\n",
    "persistIntents(intentList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Detector , Matcher and Response Selector (IDMRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultAnswerList = [\"Humm! I do not know that\", \"I am sorry I do not know that\"]\n",
    "def getToken(sentense:str)->list:\n",
    "    dialogue_words = [token.strip() for token in wtk.tokenize(sentense.lower())]\n",
    "    dialogue_words_lemmatized = [wnl.lemmatize(token) for token in dialogue_words]\n",
    "    return dialogue_words_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrainingIntents():\n",
    "    intentFile = open('intent.json', 'r')\n",
    "    intentDict = json.load(intentFile)\n",
    "    intentFile.close()\n",
    "    return intentDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatternWithHighestSimilarity(userInputTokenList:list, patternDict:dict):\n",
    "    closestMatchList = []\n",
    "    highestJackScore = 0.0\n",
    "    for lineNo in patternDict:\n",
    "        pattern = patternDict[lineNo]\n",
    "        patternToken = getToken(pattern)\n",
    "        jackScore = getJaccardSimilarity(userInputTokenList, patternToken)\n",
    "        if(jackScore > highestJackScore):\n",
    "            closestMatchList = []\n",
    "            highestJackScore = jackScore\n",
    "            closestMatchList.append(lineNo)\n",
    "        elif(jackScore == highestJackScore):\n",
    "            closestMatchList.append(lineNo)\n",
    "    return closestMatchList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sample intent dict\n",
    "\"christmas\": {\n",
    "    \"tag\": \"christmas\",\n",
    "    \"seedWords\": [\n",
    "      \"Noel\"\n",
    "    ],\n",
    "    \"pattern\": {\n",
    "      \"122179\": \"Merry Christmas eve.\"\n",
    "    },\n",
    "    \"response\": {\n",
    "      \"122179\": \"Not if you work for FedEx.\"\n",
    "    }\n",
    "  }\n",
    "'''\n",
    "\n",
    "def createUserResponse(userInput:str, userIntent:str, trainingIntent:dict):\n",
    "    userInputTokenList =  getToken(userInput)\n",
    "    intentDict = trainingIntent[userIntent]\n",
    "    patternDict = intentDict[\"pattern\"]\n",
    "    answerList = getPatternWithHighestSimilarity(userInputTokenList, patternDict)\n",
    "    if len(answerList) == 0:\n",
    "        return random.choice(defaultAnswerList)\n",
    "    else:\n",
    "        lineNo = random.choice(answerList)\n",
    "        return intentDict[\"response\"][lineNo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "myInput = [\n",
    "           {'userInput': \"is there anything bothering you?\", 'tag':'explanation'},\n",
    "           {'userInput': \"he is waiting downstairs\", 'tag':'downstairs'},\n",
    "           {'userInput': \"Hey! when is christmas?\", 'tag':'christmas'},\n",
    "           {'userInput': \"what is weather outside?\", 'tag':'holiday'},\n",
    "           {'userInput': \"I am very excited about my vacation!\", 'tag':'holiday'}]\n",
    "\n",
    "def test():\n",
    "    print(\"Brining Mov-bee online ......\")\n",
    "    print()\n",
    "    trainingIntentDict = readTrainingIntents()\n",
    "    for inputDict in myInput:\n",
    "        response = createUserResponse(inputDict['userInput'], inputDict['tag'], trainingIntentDict)\n",
    "        print(\"User: %s\"%inputDict['userInput'])\n",
    "        print(\"Bee: %s\"%response)\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML model training: RNN sequence to sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populateTrainingDataAndLabels(patternDict:dict, patternList:list ,label:list, tag:str):\n",
    "    for key in patternDict:\n",
    "        patternList.append(patternDict[key])\n",
    "        label.append(tag)\n",
    "\n",
    "def getTraininGDataAndLabel(training_intents):\n",
    "    trainingPatternList = []\n",
    "    labelList = []\n",
    "    for intent in training_intents:\n",
    "        if len(training_intents[intent]['pattern']) > 0:\n",
    "            populateTrainingDataAndLabels(training_intents[intent]['pattern'],trainingPatternList, labelList, intent)\n",
    "    return trainingPatternList, labelList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train Sequential model. For speeding up the training process I have kept only 20 epoches. With 20 epoch model has 34% accuracy. With epoch of 100 I was able to achieve accuracy of 45%. If we select topics more carefully i.e. only keep less ambigious and specific topics, instead of genric and ambigious topics then we can achieve even more accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndPickleModel():\n",
    "    training_intents = readTrainingIntents()\n",
    "    trainSentence, trainLabels = getTraininGDataAndLabel(training_intents)\n",
    "    num_classes = len(training_intents)\n",
    "\n",
    "    lbl_encoder = LabelEncoder()\n",
    "    lbl_encoder.fit(trainLabels)\n",
    "    trainLabels = lbl_encoder.transform(trainLabels)\n",
    "    vocab_size = 10000\n",
    "    embedding_dim = 16\n",
    "    max_len = 100\n",
    "    oov_token = \"<OOV>\"\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token) # adding out of vocabulary token\n",
    "    tokenizer.fit_on_texts(trainSentence)\n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(trainSentence)\n",
    "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=max_len)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_len))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    epochs = 20\n",
    "    history = model.fit(padded_sequences, np.array(trainLabels), epochs=epochs)\n",
    "    print(history)\n",
    "\n",
    "    model.save(\"chat_model\")\n",
    "    with open('tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    # to save the fitted label encoder\n",
    "    with open('label_encoder.pickle', 'wb') as ecn_file:\n",
    "        pickle.dump(lbl_encoder, ecn_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 5.3794 - accuracy: 0.0438\n",
      "Epoch 2/20\n",
      "10238/10238 [==============================] - 21s 2ms/step - loss: 4.8619 - accuracy: 0.0764\n",
      "Epoch 3/20\n",
      "10238/10238 [==============================] - 21s 2ms/step - loss: 4.6064 - accuracy: 0.1078\n",
      "Epoch 4/20\n",
      "10238/10238 [==============================] - 23s 2ms/step - loss: 4.2648 - accuracy: 0.1430\n",
      "Epoch 5/20\n",
      "10238/10238 [==============================] - 25s 2ms/step - loss: 3.9308 - accuracy: 0.1798\n",
      "Epoch 6/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 3.6191 - accuracy: 0.2198\n",
      "Epoch 7/20\n",
      "10238/10238 [==============================] - 25s 2ms/step - loss: 3.3626 - accuracy: 0.2468\n",
      "Epoch 8/20\n",
      "10238/10238 [==============================] - 25s 2ms/step - loss: 3.1726 - accuracy: 0.2668\n",
      "Epoch 9/20\n",
      "10238/10238 [==============================] - 23s 2ms/step - loss: 3.0198 - accuracy: 0.2817\n",
      "Epoch 10/20\n",
      "10238/10238 [==============================] - 22s 2ms/step - loss: 2.9046 - accuracy: 0.2948\n",
      "Epoch 11/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 2.8215 - accuracy: 0.3027\n",
      "Epoch 12/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 2.7561 - accuracy: 0.3102\n",
      "Epoch 13/20\n",
      "10238/10238 [==============================] - 23s 2ms/step - loss: 2.7035 - accuracy: 0.3164\n",
      "Epoch 14/20\n",
      "10238/10238 [==============================] - 22s 2ms/step - loss: 2.6576 - accuracy: 0.3213\n",
      "Epoch 15/20\n",
      "10238/10238 [==============================] - 20s 2ms/step - loss: 2.6151 - accuracy: 0.3257\n",
      "Epoch 16/20\n",
      "10238/10238 [==============================] - 21s 2ms/step - loss: 2.5730 - accuracy: 0.3295\n",
      "Epoch 17/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 2.5308 - accuracy: 0.3331\n",
      "Epoch 18/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 2.4900 - accuracy: 0.3378\n",
      "Epoch 19/20\n",
      "10238/10238 [==============================] - 24s 2ms/step - loss: 2.4499 - accuracy: 0.3412\n",
      "Epoch 20/20\n",
      "10238/10238 [==============================] - 22s 2ms/step - loss: 2.4103 - accuracy: 0.3447\n",
      "<keras.callbacks.History object at 0x000001D0422C3FD0>\n",
      "INFO:tensorflow:Assets written to: chat_model\\assets\n"
     ]
    }
   ],
   "source": [
    "trainAndPickleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mov-bee Chat Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    print(\"Bringing Mov-Bee online ....\")\n",
    "    # load trained model\n",
    "    model = keras.models.load_model('chat_model')\n",
    "\n",
    "    # load tokenizer object\n",
    "    with open('tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    # load label encoder object\n",
    "    with open('label_encoder.pickle', 'rb') as enc:\n",
    "        lbl_encoder = pickle.load(enc)\n",
    "\n",
    "    # parameters\n",
    "    max_len = 20\n",
    "    trainingIntentDict = readTrainingIntents()\n",
    "    while True:\n",
    "        print(Fore.LIGHTBLUE_EX + \"User: \" + Style.RESET_ALL, end=\"\")\n",
    "        userInput = input()\n",
    "        if userInput.lower() == \"quit\" or userInput.lower() == \"exit\" or userInput.lower() == \"end\":\n",
    "            break\n",
    "\n",
    "        result = model.predict(keras.preprocessing.sequence.pad_sequences(tokenizer.texts_to_sequences([userInput]),\n",
    "                                                                          truncating='post', maxlen=max_len))\n",
    "        tag = lbl_encoder.inverse_transform([np.argmax(result)])\n",
    "\n",
    "        response = createUserResponse(userInput, tag[0], trainingIntentDict)\n",
    "        print(Fore.GREEN + \"ChatBot:\" + Style.RESET_ALL, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mStart messaging with the bot (type quit, exit or end to stop)!\u001b[0m\n",
      "Bringing Mov-Bee online ....\n",
      "\u001b[94mUser: \u001b[0m"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-5f2fab66f29b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mYELLOW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"Start messaging with the bot (type quit, exit or end to stop)!\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mStyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-33-88c33d2613b9>\u001b[0m in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLIGHTBLUE_EX\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"User: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mStyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0muserInput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0muserInput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"quit\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0muserInput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"exit\"\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0muserInput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "print(Fore.YELLOW + \"Start messaging with the bot (type quit, exit or end to stop)!\" + Style.RESET_ALL)\n",
    "chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brining Mov-bee online ......\n",
      "\n",
      "User: is there anything bothering you?\n",
      "Bee: Look, Dave, I know that you're sincere and that you're trying to do a competent job, and that you're trying to be helpful, but I can assure the problem is with the AO-units, and with your test gear.\n",
      "\n",
      "\n",
      "User: he is waiting downstairs\n",
      "Bee: Where is Pimenov? Where is Pimenov?\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'christmas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-fbd55f77ab7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-28-5b69215a881d>\u001b[0m in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtrainingIntentDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadTrainingIntents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minputDict\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmyInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreateUserResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userInput'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tag'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingIntentDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"User: %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0minputDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userInput'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Bee: %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-d97c91ff98c1>\u001b[0m in \u001b[0;36mcreateUserResponse\u001b[1;34m(userInput, userIntent, trainingIntent)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreateUserResponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserInput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muserIntent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingIntent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0muserInputTokenList\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mgetToken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserInput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mintentDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainingIntent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0muserIntent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mpatternDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mintentDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"pattern\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0manswerList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPatternWithHighestSimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muserInputTokenList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatternDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'christmas'"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "##### I have implemented the code for Chatbot in another IDE(pyCharm). However, while integrating with jupyter notebook I was facing some issue. I have provided a sample chatbot that I have tested upon. But, I will include the snapshot from pycharm for the evaluation part in my report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
